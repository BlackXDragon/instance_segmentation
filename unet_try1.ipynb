{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net implementation in Pytorch\n",
    "# to train on the Ultralytics package-seg dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# package_seg\n",
    "# - test\n",
    "#   - images\n",
    "#   - labels\n",
    "# - train\n",
    "#   - images\n",
    "#   - labels\n",
    "# - valid\n",
    "#   - images\n",
    "#   - labels\n",
    "class PackageSegDataset(Dataset):\n",
    "\tdef __init__(self, root_dir, transform=None):\n",
    "\t\tself.root_dir = root_dir\n",
    "\t\tself.transform = transform\n",
    "\t\tself.train_dir = os.path.join(root_dir, 'train')\n",
    "\t\tself.val_dir = os.path.join(root_dir, 'valid')\n",
    "\t\tself.test_dir = os.path.join(root_dir, 'test')\n",
    "\t\tself.train_images = os.listdir(os.path.join(self.train_dir, 'images'))\n",
    "\t\tself.val_images = os.listdir(os.path.join(self.val_dir, 'images'))\n",
    "\t\tself.test_images = os.listdir(os.path.join(self.test_dir, 'images'))\n",
    "  \n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.train_images) + len(self.val_images) + len(self.test_images)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tif idx < len(self.train_images):\n",
    "\t\t\timg_path = os.path.join(self.train_dir, 'images', self.train_images[idx])\n",
    "\t\t\tlabel_path = os.path.join(self.train_dir, 'labels', self.train_images[idx].replace('.jpg', '.txt'))\n",
    "\t\telif idx < len(self.train_images) + len(self.val_images):\n",
    "\t\t\tidx -= len(self.train_images)\n",
    "\t\t\timg_path = os.path.join(self.val_dir, 'images', self.val_images[idx])\n",
    "\t\t\tlabel_path = os.path.join(self.val_dir, 'labels', self.val_images[idx].replace('.jpg', '.txt'))\n",
    "\t\telse:\n",
    "\t\t\tidx -= len(self.train_images) + len(self.val_images)\n",
    "\t\t\timg_path = os.path.join(self.test_dir, 'images', self.test_images[idx])\n",
    "\t\t\tlabel_path = os.path.join(self.test_dir, 'labels', self.test_images[idx].replace('.jpg', '.txt'))\n",
    "\t\timage = cv2.imread(img_path)\n",
    "\t\tlabel = []\n",
    "\t\twith open(label_path, 'r') as f:\n",
    "\t\t\tlabel_text = f.read()\n",
    "\t\t\tfor line in label_text.split('\\n'):\n",
    "\t\t\t\tif line == '':\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tpoint_list = list(map(float, line.split(' ')[1:]))\n",
    "\t\t\t\tpoints = []\n",
    "\t\t\t\tfor i in range(0, len(point_list), 2):\n",
    "\t\t\t\t\tpoints.append((point_list[i], point_list[i+1]))\n",
    "\t\t\t\tlabel.append({\n",
    "                  \t'label': int(line.split(' ')[0]),\n",
    "\t\t\t\t\t'points': np.array(points)\n",
    "                })\n",
    "\t\tmask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "\t\tfor l in label:\n",
    "\t\t\tcv2.fillPoly(mask, [l['points'].astype(np.int32)], l['label']+1)\n",
    "\t\tmask = mask[..., np.newaxis]\n",
    "  \n",
    "\t\tbboxes = []\n",
    "\t\tfor l in label:\n",
    "\t\t\txmin, ymin = np.min(l['points'], axis=0)\n",
    "\t\t\txmax, ymax = np.max(l['points'], axis=0)\n",
    "\t\t\tbboxes.append([xmin, ymin, xmax, ymax, l['label']])\n",
    "  \n",
    "\t\tif self.transform:\n",
    "\t\t\taugs = self.transform(image=image, mask=mask, bboxes=bboxes)\n",
    "\t\t\timage = augs['image']\n",
    "\t\t\tmask = augs['mask']\n",
    "\t\t\tbboxes = augs['bboxes']\n",
    "\t\treturn image, mask, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the U-Net model for instance segmentation\n",
    "# The model requires 2 output heads: one for segmentation masks and one for bounding boxes\n",
    "class UNet(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(UNet, self).__init__()\n",
    "\n",
    "\t\t# Encoder\n",
    "\t\tself.conv1_1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "\t\tself.conv1_2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "\t\tself.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "\t\tself.conv2_1 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "\t\tself.conv2_2 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "\t\tself.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "\t\tself.conv3_1 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "\t\tself.conv3_2 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "\t\tself.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "\t\tself.conv4_1 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "\t\tself.conv4_2 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "\t\tself.pool4 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "\t\tself.conv5_1 = nn.Conv2d(512, 1024, 3, padding=1)\n",
    "\t\tself.conv5_2 = nn.Conv2d(1024, 1024, 3, padding=1)\n",
    "\n",
    "\t\t# Decoder\n",
    "\t\tself.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
    "\t\tself.conv4_3 = nn.Conv2d(1024, 512, 3, padding=1)\n",
    "\t\tself.conv4_4 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "\n",
    "\t\tself.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "\t\tself.conv3_3 = nn.Conv2d(512, 256, 3, padding=1)\n",
    "\t\tself.conv3_4 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "\n",
    "\t\tself.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "\t\tself.conv2_3 = nn.Conv2d(256, 128, 3, padding=1)\n",
    "\t\tself.conv2_4 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "  \n",
    "\t\tself.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "\t\tself.conv1_3 = nn.Conv2d(128, 64, 3, padding=1)\n",
    "\t\tself.conv1_4 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "\n",
    "\t\t# Segmentation head\n",
    "\t\tself.conv1_5 = nn.Conv2d(64, 1, 1)\n",
    "  \n",
    "\t\t# Bounding box head\n",
    "\t\tself.fc1 = nn.Linear(1024, 512)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t# Encoder\n",
    "\t\tx1 = F.relu(self.conv1_1(x))\n",
    "\t\tx1 = F.relu(self.conv1_2(x1))\n",
    "\t\tx2 = self.pool1(x1)\n",
    "\n",
    "\t\tx2 = F.relu(self.conv2_1(x2))\n",
    "\t\tx2 = F.relu(self.conv2_2(x2))\n",
    "\t\tx3 = self.pool2(x2)\n",
    "\n",
    "\t\tx3 = F.relu(self.conv3_1(x3))\n",
    "\t\tx3 = F.relu(self.conv3_2(x3))\n",
    "\t\tx4 = self.pool3(x3)\n",
    "\n",
    "\t\tx4 = F.relu(self.conv4_1(x4))\n",
    "\t\tx4 = F.relu(self.conv4_2(x4))\n",
    "\t\tx5 = self.pool4(x4)\n",
    "\n",
    "\t\tx5 = F.relu(self.conv5_1(x5))\n",
    "\t\tx5 = F.relu(self.conv5_2(x5))\n",
    "\n",
    "\t\t# Decoder\n",
    "\t\tx = self.upconv4(x5)\n",
    "\t\tx = torch.cat((x4, x), dim=1)\n",
    "\t\tx = F.relu(self.conv4_3(x))\n",
    "\t\tx = F.relu(self.conv4_4(x))\n",
    "\n",
    "\t\tx = self.upconv3(x)\n",
    "\t\tx = torch.cat((x3, x), dim=1)\n",
    "\t\tx = F.relu(self.conv3_3(x))\n",
    "\t\tx = F.relu(self.conv3_4(x))\n",
    "\n",
    "\t\tx = self.upconv2(x)\n",
    "\t\tx = torch.cat((x2, x), dim=1)\n",
    "\t\tx = F.relu(self.conv2_3(x))\n",
    "\t\tx = F.relu(self.conv2_4(x))\n",
    "\n",
    "\t\tx = self.upconv1(x)\n",
    "\t\tx = torch.cat((x1, x), dim=1)\n",
    "\t\tx = F.relu(self.conv1_3(x))\n",
    "\t\tx = F.relu(self.conv1_4(x))\n",
    "\n",
    "\t\t# Segmentation head\n",
    "\t\tseg = self.conv1_5(x)\n",
    "\t\tseg = F.sigmoid(seg)\n",
    "  \n",
    "\t\t# Bounding box head\n",
    "\t\tx = F.adaptive_avg_pool2d(x5, (1, 1))\n",
    "\t\tx = x.view(x.size(0), -1)\n",
    "\t\tbbox = self.fc1(x)\n",
    "  \n",
    "\t\treturn seg, bbox\n",
    "\n",
    "# Define the loss function\n",
    "class DiceLoss(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(DiceLoss, self).__init__()\n",
    "\n",
    "\tdef forward(self, pred, target):\n",
    "\t\tsmooth = 1e-6\n",
    "\t\tintersection = (pred * target).sum()\n",
    "\t\tdenominator = pred.sum() + target.sum()\n",
    "\n",
    "\t\treturn 1 - (2 * intersection + smooth) / (denominator + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 2197\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset\n",
    "transform = A.Compose([\n",
    "\tA.Resize(256, 256),\n",
    "\tA.Normalize(mean=(0, 0, 0), std=(1, 1, 1)),\n",
    "], bbox_params=A.BboxParams(format='pascal_voc'))\n",
    "\n",
    "dataset = PackageSegDataset('data/package_seg', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "print(f\"Number of images: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first 3 images and labels\n",
    "fig, ax = plt.subplots(3, 2, figsize=(10, 10))\n",
    "for i in range(3):\n",
    "\timg, mask, bboxes = dataset[i]\n",
    " \n",
    "\timg = np.array(img * 255, dtype=np.uint8)\n",
    "\t\n",
    "\t# Apply the segmentation mask and bounding boxes\n",
    "\tmask = mask.squeeze()\n",
    "\tmask = np.array(mask, dtype=np.uint8)\n",
    "\t\n",
    "\t# Apply different colors for each label\n",
    "\tunique_labels = np.unique(mask)\n",
    "\tcolors = np.random.randint(0, 255, size=(len(unique_labels), 3))\n",
    "\tcolor_map = {int(label): color for label, color in zip(unique_labels, colors)}\n",
    "\tdel color_map[0]\n",
    "\tcolor_mask = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "\tfor label, color in color_map.items():\n",
    "\t\tcolor_mask[mask == label] = color\n",
    "\n",
    "\t# Overlay the mask on the image\n",
    "\tmask = cv2.addWeighted(img, 0.5, color_mask, 0.5, 0)\n",
    " \n",
    "\tfor bbox in bboxes:\n",
    "\t\txmin, ymin, xmax, ymax, label = bbox\n",
    "\t\tcv2.rectangle(mask, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 255, 0), 2)\n",
    "\t\tcv2.putText(mask, str(label), (int(xmin), int(ymin)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "  \n",
    "\tax[i, 0].imshow(img)\n",
    "\tax[i, 0].axis('off')\n",
    "\tax[i, 0].set_title('Image')\n",
    " \n",
    "\tax[i, 1].imshow(mask)\n",
    "\tax[i, 1].axis('off')\n",
    "\tax[i, 1].set_title('Mask')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
